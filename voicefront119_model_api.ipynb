{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ry1IUj9z3mdG",
        "outputId": "67132527-a049-4618-aef8-b77010da9cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nest-asyncio pyngrok fastapi uvicorn python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YQpvbSJ4F4G"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoTokenizer, ElectraModel\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile, WebSocket, WebSocketDisconnect\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from tempfile import NamedTemporaryFile\n",
        "import uvicorn\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yyRiLhvi5B98",
        "outputId": "507c9504-8c26-463c-95d2-6f41ae29fe9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# 파인튜닝된 whisper 모델 로드\n",
        "lora_model_name = \"/content/drive/MyDrive/whisper_m__250512_001\"  # LoRA 모델 경로\n",
        "lora_model = WhisperForConditionalGeneration.from_pretrained(lora_model_name)\n",
        "lora_processor = WhisperProcessor.from_pretrained(lora_model_name)\n",
        "# 추론 모드로 전환\n",
        "lora_model.eval()\n",
        "\n",
        "# 긴급도 분류 모델 로드\n",
        "urgency_model = tf.keras.models.load_model(\"/content/drive/MyDrive/urgency_classification_001.h5\")\n",
        "\n",
        "# 긴급도 분류 모델을 위한 tokenizer 및 임베딩 모델 로드\n",
        "urgency_tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "urgency_embedding_model = ElectraModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "urgency_embedding_model.eval()\n",
        "\n",
        "# 텍스트 정제, 토큰화, 임베딩\n",
        "def text_processing(text: str):\n",
        "    stopwords = [\"개인정보\"]\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"(\\b\\w+\\b)( \\1)+\", r\"\\1\", text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "\n",
        "    clean_text = ' '.join(words)\n",
        "\n",
        "    inputs = urgency_tokenizer(\n",
        "        clean_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = urgency_embedding_model(**inputs)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embedding.squeeze().numpy().tolist()\n",
        "\n",
        "# 긴급도 분류\n",
        "def classify_urgency(text, disaster_type):\n",
        "    text_input = text_processing(text)  # 임베딩\n",
        "    text_input = np.array(text_input).reshape(1, -1)  # 2D 배열로 변환\n",
        "\n",
        "    disaster_type = np.array(disaster_type).reshape(1, -1)  # 2D로 변환\n",
        "\n",
        "    # 예측\n",
        "    pred = urgency_model.predict({\"text_input\": text_input, \"disaster_input\": disaster_type})\n",
        "    pred_class = np.argmax(pred, axis=1)[0]\n",
        "\n",
        "    # 클래스 인덱스 상중하로 매핑\n",
        "    urgency_label = {0: '하', 1: '중', 2: '상'}\n",
        "    return urgency_label[pred_class]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import openai\n",
        "openai.api_key = getpass.getpass(prompt = 'OpenAI API키 입력')"
      ],
      "metadata": {
        "id": "4nnu835iUKw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da688e14-fffd-40c5-81b7-de7223e00d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API키 입력··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QDmlJFv4F7n"
      },
      "outputs": [],
      "source": [
        "app = FastAPI(\n",
        "    title=\"VoiceFront119\",\n",
        "    version=\"1.0\",\n",
        "    decsription=\"API Server\"\n",
        ")\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "# CORS 미들웨어 추가\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # 모든 origin 허용\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# FastAPI 연계 확인\n",
        "class FileInput(BaseModel):\n",
        "    file_path: str\n",
        "\n",
        "class UrgencyInput(BaseModel):\n",
        "    text: str\n",
        "    disaster_type: List[int]\n",
        "\n",
        "class GptInput(BaseModel):\n",
        "  text: str\n",
        "\n",
        "# 파인튜닝된 Whisper 모델로 STT 결과 반환\n",
        "# 전체 wav 파일 20초 단위로 잘라서 텍스트 변환\n",
        "@app.post(\"/stt_result\", status_code=200)\n",
        "async def stt_from_audio(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # file_path = input.file_path\n",
        "        print(\"Received file:\", file.filename)\n",
        "\n",
        "        # 임시 파일로 저장\n",
        "        with NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
        "            tmp.write(await file.read())\n",
        "            tmp_path = tmp.name\n",
        "\n",
        "        # torchaudio로 오디오 로딩\n",
        "        speech_array, original_sampling_rate = torchaudio.load(tmp_path)\n",
        "        os.remove(tmp_path)\n",
        "\n",
        "        sampling_rate = 16000\n",
        "\n",
        "        # 16kHz로 리샘플링\n",
        "        if original_sampling_rate != sampling_rate:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=original_sampling_rate, new_freq=sampling_rate)\n",
        "            speech_array = resampler(speech_array)\n",
        "\n",
        "        chunk_duration = 20 # sec\n",
        "        chunk_size = chunk_duration * sampling_rate\n",
        "        full_transcription = []\n",
        "\n",
        "        # 20초 단위로 슬라이딩\n",
        "        for start in range(0, speech_array.shape[1], chunk_size):\n",
        "            end = start + chunk_size\n",
        "            chunk = speech_array[:, start:end]\n",
        "\n",
        "            # 너무 짧은 chunk는 생략\n",
        "            if chunk.shape[1] < sampling_rate * 2:\n",
        "                continue\n",
        "\n",
        "            # 입력 처리\n",
        "            inputs = lora_processor(\n",
        "                chunk.squeeze(),\n",
        "                sampling_rate=sampling_rate,\n",
        "                return_tensors=\"pt\",\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "\n",
        "            # 추론\n",
        "            with torch.no_grad():\n",
        "                predicted_ids = lora_model.generate(\n",
        "                    inputs[\"input_features\"],\n",
        "                    attention_mask=inputs[\"attention_mask\"],\n",
        "                    language=\"ko\"\n",
        "                )\n",
        "\n",
        "            # 디코딩\n",
        "            transcription = lora_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "            full_transcription.append(transcription)\n",
        "\n",
        "        stt_result = \" \".join(full_transcription)\n",
        "\n",
        "        return {\"stt_result\": stt_result}\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "# # 클라이언트로부터 잘라진 wav 파일 받아와서 지속적으로 변환\n",
        "# @app.websocket(\"/ws_stt\")\n",
        "# async def websocket_endpoint(websocket: WebSocket):\n",
        "#     await websocket.accept()\n",
        "\n",
        "#     try:\n",
        "#         while True:\n",
        "#             data = await websocket.receive_bytes()\n",
        "#             with NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
        "#                 tmp.write(data)\n",
        "#                 tmp_path = tmp.name\n",
        "\n",
        "#             # 여기서 STT 수행\n",
        "#             result = wav_to_text(tmp_path)\n",
        "#             os.remove(tmp_path)\n",
        "\n",
        "#             await websocket.send_text(result)\n",
        "#     except Exception as e:\n",
        "#         print(\"연결 종료:\", e)\n",
        "#     finally:\n",
        "#         await websocket.close()\n",
        "\n",
        "\n",
        "# 긴급도 분류\n",
        "@app.post(\"/urgency_classification\")\n",
        "async def urgency_classification(input: UrgencyInput):\n",
        "    print('Input received.')\n",
        "    try:\n",
        "        result = classify_urgency(input.text, input.disaster_type)\n",
        "        return {\"urgency_level\": result}\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "# 신고내용 요약 및 분석\n",
        "@app.post(\"/gpt_response\")\n",
        "async def gpt4_turbo_response(input: GptInput):\n",
        "    print('Input received.')\n",
        "    response = openai.chat.completions.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[{\"role\": \"system\",\n",
        "                       \"content\":\"\"\"\n",
        "                        당신은 119 대원들의 신고 접수를 돕는 시스템이다.\n",
        "                        신고 내용을 기반으로 핵심 정보를 요약하고 119 출동 대원이 빠르게 파악할 수 있도록 정리해야 한다.\n",
        "\n",
        "                        작성 지침 :\n",
        "                        - 모든 항목은 아이템 형태(• 또는 -)로 간결하게 작성한다.\n",
        "                        - 신고 위치와 신고 내용은 반드시 포함되어야 한다.\n",
        "                        - 신고 내용의 첫번째 줄에서는 문제 상황을 한줄로 간단히 정의한다.\n",
        "\n",
        "                        - 구조/구급 상황일 경우:\n",
        "                          - 환자와 신고자의 관계\n",
        "                          - 환자의 상태 또는 증상 등\n",
        "                        - 화재 상황일 경우:\n",
        "                          - 화재 발생 위치\n",
        "                          - 현재 진행 상황 (연기, 화염, 대피 여부 등) 등\n",
        "\n",
        "                        - 주어지지 않은 정보를 허위로 작성해선 안 된다.\n",
        "                        - 그 외 유의사항이나 추가 확인이 필요한 정보는 \"비고\" 항목에 명확히 정리한다.\n",
        "                        - 모든 내용은 한국어로 작성하고, 출동 대원이 즉시 파악할 수 있도록 한다.\n",
        "                        - 볼드나 기울임 등 마크다운은 사용하지 않는다.\n",
        "                        \"\"\"\n",
        "                       },\n",
        "                      {\"role\": \"user\",\n",
        "                       \"content\": input.text\n",
        "                       }],\n",
        "            # max_tokens=2000,\n",
        "            temperature=0.7,\n",
        "            frequency_penalty=0.7\n",
        "        )\n",
        "\n",
        "    gpt_response = response.choices[0].message.content.strip()\n",
        "    return {\"gpt_response\": gpt_response}\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"VoiceFront119 Model API is running\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUIFS0gd4z8S",
        "outputId": "4826cd5c-dcfa-4111-e9a9-11655e00eff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "공용 URL https://c0e4-35-204-127-207.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [608]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received file: 광주_화재.wav\n",
            "INFO:     218.147.225.131:0 - \"POST /stt_result HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "INFO:     218.147.225.131:0 - \"POST /gpt_response HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "INFO:     218.147.225.131:0 - \"POST /urgency_classification HTTP/1.1\" 200 OK\n",
            "Received file: 광주_화재.wav\n",
            "INFO:     218.147.225.131:0 - \"POST /stt_result HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "INFO:     218.147.225.131:0 - \"POST /urgency_classification HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "INFO:     218.147.225.131:0 - \"POST /gpt_response HTTP/1.1\" 200 OK\n",
            "Received file: 광주_화재.wav\n",
            "INFO:     218.147.225.131:0 - \"POST /stt_result HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "INFO:     218.147.225.131:0 - \"POST /urgency_classification HTTP/1.1\" 200 OK\n",
            "Input received.\n",
            "INFO:     218.147.225.131:0 - \"POST /gpt_response HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [608]\n"
          ]
        }
      ],
      "source": [
        "# ngrok 인증 토큰과 포트 설정\n",
        "auth_token = \"ngrok 인증 토큰 입력\"\n",
        "ngrok.set_auth_token(auth_token)\n",
        "# ngrok 연결\n",
        "ngrokTunnel = ngrok.connect(8000)\n",
        "print(\"공용 URL\", ngrokTunnel.public_url)\n",
        "# 비동기 처리를 위한 nest_asyncio 적용\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000, timeout_keep_alive=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPtL840q40He"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}